# Common environment setting
spark.pyspark.python=python3.11
spark.pyspark.driver=python3.11
spark.local.dir=/tmp/spark-tmp
spark.jars.ivy=/opt/spark/.ivy2
spark.jars.repositories=https://repo1.maven.org/maven2/,https://maven-central.storage-download.googleapis.com/maven2/
spark.executor.extraJavaOptions=-Duser.name=spark
spark.driver.extraJavaOptions=-Duser.name=spark

# AWS S3 setting
spark.hadoop.fs.defaultFS=s3a://w-userflow-featurestore/
spark.hadoop.fs.s3a.endpoint=s3.ap-northeast-2.amazonaws.com
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.connection.maximum=200

# Iceberg setting
spark.sql.defaultCatalog=iceberg
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog                 # catalog name
spark.sql.catalog.iceberg.type=hadoop                                           # catalog backend type
spark.sql.catalog.iceberg.warehouse=s3a://w-userflow-featurestore/iceberg/
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# Spark performance tuning
spark.sql.adaptive.enabled=true
spark.sql.adaptive.localShuffleReader.enabled=true
spark.sql.adaptive.coalescePartitions.enabled=true
